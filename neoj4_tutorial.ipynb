{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./arxiv-metadata-oai-snapshot.json\"\n",
    "\n",
    "metadata  = []\n",
    "\n",
    "lines = 100000    # 100k for testing\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    \n",
    "    for line in tqdm(f):\n",
    "        metadata.append(json.loads(line))\n",
    "        lines -= 1\n",
    "        if lines == 0: break\n",
    "            \n",
    "df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author_list(line):\n",
    "    # Cleans author dataframe column, creating a list of authors in the row.\n",
    "    return [e[1] + ' ' + e[0] for e in line]\n",
    "\n",
    "\n",
    "def get_category_list(line):\n",
    "    # Cleans category dataframe column, creating a list of categories in the row.\n",
    "    return list(line.split(\" \"))\n",
    "\n",
    "\n",
    "df['cleaned_authors_list'] = df['authors_parsed'].map(get_author_list)\n",
    "df['category_list'] = df['categories'].map(get_category_list)\n",
    "df = df.drop(['submitter', 'authors', \n",
    "             'comments', 'journal-ref', \n",
    "             'doi', 'report-no', 'license', \n",
    "             'versions', 'update_date', \n",
    "             'abstract', 'authors_parsed', \n",
    "             'categories'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, passwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = passwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to Neo4j\n",
    "conn = Neo4jConnection(uri=\"bolt://127.0.0.1:7687\", \n",
    "                       user=\"neo4j\",              \n",
    "                       passwd=\"cutepuppies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some constraints to ensure the nodes aren't duplicates\n",
    "conn.query('CREATE CONSTRAINT papers IF NOT EXISTS FOR (p:Paper) REQUIRE p.id IS UNIQUE')\n",
    "conn.query('CREATE CONSTRAINT authors IF NOT EXISTS FOR (a:Author) REQUIRE a.name IS UNIQUE')\n",
    "conn.query('CREATE CONSTRAINT categories IF NOT EXISTS FOR (c:Category) REQUIRE c.category IS UNIQUE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_categories(categories):\n",
    "    # Adds category nodes to the Neo4j graph.\n",
    "    query = '''\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (c:Category {category: row.category})\n",
    "            RETURN count(*) as total\n",
    "            '''\n",
    "    return conn.query(query, parameters = {'rows':categories.to_dict('records')})\n",
    "\n",
    "\n",
    "def add_authors(rows, batch_size=10000):\n",
    "    # Adds author nodes to the Neo4j graph as a batch job.\n",
    "    query = '''\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (:Author {name: row.author})\n",
    "            RETURN count(*) as total\n",
    "            '''\n",
    "    return insert_data(query, rows, batch_size)\n",
    "\n",
    "\n",
    "def insert_data(query, rows, batch_size = 10000):\n",
    "    # Function to handle the updating the Neo4j database in batch mode.\n",
    "    \n",
    "    total = 0\n",
    "    batch = 0\n",
    "    start = time.time()\n",
    "    result = None\n",
    "    \n",
    "    while batch * batch_size < len(rows):\n",
    "\n",
    "        res = conn.query(query, \n",
    "                         parameters = {'rows': rows[batch*batch_size:(batch+1)*batch_size].to_dict('records')})\n",
    "        total += res[0]['total']\n",
    "        batch += 1\n",
    "        result = {\"total\":total, \n",
    "                  \"batches\":batch, \n",
    "                  \"time\":time.time()-start}\n",
    "        print(result)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_papers(rows, batch_size=5000):\n",
    "   # Adds paper nodes and (:Author)--(:Paper) and \n",
    "   # (:Paper)--(:Category) relationships to the Neo4j graph as a \n",
    "   # batch job.\n",
    " \n",
    "   query = '''\n",
    "   UNWIND $rows as row\n",
    "   MERGE (p:Paper {id:row.id}) ON CREATE SET p.title = row.title\n",
    " \n",
    "   // connect categories\n",
    "   WITH row, p\n",
    "   UNWIND row.category_list AS category_name\n",
    "   MATCH (c:Category {category: category_name})\n",
    "   MERGE (p)-[:IN_CATEGORY]->(c)\n",
    " \n",
    "   // connect authors\n",
    "   WITH distinct row, p // reduce cardinality\n",
    "   UNWIND row.cleaned_authors_list AS author\n",
    "   MATCH (a:Author {name: author})\n",
    "   MERGE (a)-[:AUTHORED]->(p)\n",
    "   RETURN count(distinct p) as total\n",
    "   '''\n",
    " \n",
    "   return insert_data(query, rows, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = pd.DataFrame(df[['category_list']])\n",
    "categories.rename(columns={'category_list':'category'},\n",
    "                  inplace=True)\n",
    "categories = categories.explode('category') \\\n",
    "                       .drop_duplicates(subset=['category'])\n",
    "\n",
    "authors = pd.DataFrame(df[['cleaned_authors_list']])\n",
    "authors.rename(columns={'cleaned_authors_list':'author'},\n",
    "               inplace=True)\n",
    "authors=authors.explode('author').drop_duplicates(subset=['author'])\n",
    "\n",
    "add_categories(categories)\n",
    "add_authors(authors)\n",
    "add_papers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''\n",
    "MATCH (c:Category)\n",
    "RETURN c.category, apoc.node.degree(c, \"<IN_CATEGORY\") AS inDegree \n",
    "ORDER BY inDegree DESC \n",
    "'''\n",
    "\n",
    "top_cat_df = pd.DataFrame([dict(_) for _ in conn.query(query_string)])\n",
    "top_cat_df.head(20)\n",
    "\n",
    "\n",
    "result = conn.query(query_string)\n",
    "print(len(result))\n",
    "for record in result:\n",
    "    print(record['c.category'], record['inDegree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=top_cat_df['c.category'], y=top_cat_df['inDegree'])\n",
    "plt.xlabel('Category Name', fontsize=18)\n",
    "plt.ylabel('inDegree',fontsize=18)\n",
    "plt.xticks(rotation='vertical', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record sum(apoc.node.degree(c, \"<IN_CATEGORY\"))=7891>]\n"
     ]
    }
   ],
   "source": [
    "query_string = '''\n",
    "MATCH (c:Category)\n",
    "WHERE c.category STARTS WITH 'cs'\n",
    "RETURN sum(apoc.node.degree(c, \"<IN_CATEGORY\"))\n",
    "'''\n",
    "\n",
    "top_cat_df = pd.DataFrame([dict(_) for _ in conn.query(query_string)])\n",
    "top_cat_df.head(20)\n",
    "\n",
    "\n",
    "result = conn.query(query_string)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
